---
title: "Natural Language Processing(NLP)(1)——Text segmentation(R vs. python)"
author: "余文华"
date: "2017年1月15日"
output: html_document
---



<div id="自然语言处理natural-language-processing" class="section level2">
<h2>自然语言处理（Natural Language Processing）</h2>
<p>自然语言处理（NLP）是机器学习重要分支之一，主要应用于篇章理解、文本摘要、情感分析、知识图谱、文本翻译等领域。而NLP应用首先是对文本进行分词，当前中文分词器有Ansj、paoding、盘古分词等多种，而最基础的分词器应该属于jieba分词器。下面将分别应用R和python对jieba分词器在中文分词、词性标注和关键词提取领域的应用进行比较。</p>
</div>
<div id="一jiaba中文分词" class="section level2">
<h2>一、jiaba中文分词</h2>
<div id="r实现" class="section level3">
<h3>R实现</h3>
<pre><code>通过函数worker()来初始化分词引擎，使用segment()进行分词。有四种分词模式:最大概率法（MP）、隐马尔科夫模型（HMM）、混合模型（Mix）及索引模型（query）,默认为混合模型。具体可查看help(worker).</code></pre>
<pre class="r"><code>#install.packages(&#39;jiebaR&#39;)
library(jiebaR)
mixseg &lt;- worker()
segment( &quot;这是一段测试文本&quot; , mixseg ) 
#或者用以下操作
mixseg[&#39;这是一段测试文本&#39;]
mixseg &lt;= &quot;这是一段测试文本&quot;</code></pre>
</div>
<div id="python实现" class="section level3">
<h3>python实现</h3>
<pre><code>python中需安装jieba库，运用jieba.cut实现分词。cut_all参数为分词类型，默认为精确模式。</code></pre>
<pre class="python"><code>import jieba
seg_list = jieba.cut(u&quot;这是一段测试文本&quot;,cut_all = False)
print(&quot;Full mode: &quot;+ &quot;,&quot;.join(seg_list))  #默认精确模式</code></pre>
<pre><code>无论是R还是python都为utf—8编码。</code></pre>
</div>
</div>
<div id="二词性标注" class="section level2">
<h2>二、词性标注</h2>
<div id="r实现-1" class="section level3">
<h3>R实现</h3>
<pre><code>可以使用&lt;=.tagger 或者tag 来进行分词和词性标注，词性标注使用混合模型模型分词，标注采用和 ictclas 兼容的标记法。</code></pre>
<pre class="r"><code>words = &quot;我爱北京天安门&quot;
tagger = worker(&quot;tag&quot;) #开启词性标注启发器
tagger &lt;= words

    #    r        v       ns       ns 
    # &quot;我&quot;     &quot;爱&quot;   &quot;北京&quot; &quot;天安门&quot; </code></pre>
</div>
<div id="python实现-1" class="section level3">
<h3>python实现</h3>
<pre class="python"><code>#词性标注
import jieba.posseg as pseg
words = pseg.cut(&quot;我爱北京天安门&quot;)
for word,flag in words:
    print(&#39;%s, %s&#39; %(word,flag))
    
&lt;!-- 我, r --&gt;
&lt;!-- 爱, v --&gt;
&lt;!-- 北京, ns --&gt;
&lt;!-- 天安门, ns --&gt;</code></pre>
</div>
</div>
<div id="三关键词提取" class="section level2">
<h2>三、关键词提取</h2>
<div id="r实现-2" class="section level3">
<h3>R实现</h3>
<pre><code>R关键词提取使用逆向文件频率（IDF）文本语料库,通过worker参数“keywords”开启关键词提取启发器，topn参数为关键词的个数。</code></pre>
<pre class="r"><code>keys = worker(&quot;keywords&quot;,topn = 5, idf = IDFPATH)
keys &lt;= &quot;会议邀请到美国密歇根大学(University of Michigan, Ann Arbor）环境健康科学系副教授奚传武博士作题为“Multibarrier approach for safe drinking waterin the US : Why it failed in Flint”的学术讲座，介绍美国密歇根Flint市饮用水污染事故的发生发展和处置等方面内容。讲座后各相关单位同志与奚传武教授就生活饮用水在线监测系统、美国水污染事件的处置方式、生活饮用水老旧管网改造、如何有效减少消毒副产物以及美国涉水产品和二次供水单位的监管模式等问题进行了探讨和交流。本次交流会是我市生活饮用水卫生管理工作洽商机制运行以来的又一次新尝试，也为我市卫生计生综合监督部门探索生活饮用水卫生安全管理模式及突发水污染事件的应对措施开拓了眼界和思路。&quot;

#结果：
#        48.8677        23.4784        22.1402         20.326        18.5354 
#       &quot;饮用水&quot;        &quot;Flint&quot;         &quot;卫生&quot;       &quot;水污染&quot;         &quot;生活&quot; </code></pre>
</div>
<div id="python实现-2" class="section level3">
<h3>python实现</h3>
<pre><code>python实现关键词提取可运用TF-IDF方法和TextRank方法。allowPOS参数为限定范围词性类型。</code></pre>
<pre class="python"><code>#关键词提取
import jieba.analyse

content = u&#39;会议邀请到美国密歇根大学(University of Michigan, Ann Arbor）环境健康科学系副教授奚传武博士作题为“Multibarrier approach for safe drinking waterin the US : Why it failed in Flint”的学术讲座，介绍美国密歇根Flint市饮用水污染事故的发生发展和处置等方面内容。讲座后各相关单位同志与奚传武教授就生活饮用水在线监测系统、美国水污染事件的处置方式、生活饮用水老旧管网改造、如何有效减少消毒副产物以及美国涉水产品和二次供水单位的监管模式等问题进行了探讨和交流。本次交流会是我市生活饮用水卫生管理工作洽商机制运行以来的又一次新尝试，也为我市卫生计生综合监督部门探索生活饮用水卫生安全管理模式及突发水污染事件的应对措施开拓了眼界和思路。&#39;

#基于TF-IDF
keywords = jieba.analyse.extract_tags(content,topK = 5,withWeight = True,allowPOS = (&#39;n&#39;,&#39;nr&#39;,&#39;ns&#39;))
for item in keywords:
    print item[0],item[1]
   
#基于TF-IDF结果
# 饮用水 0.448327672795
# Flint 0.219353532163
# 卫生 0.203120821773
# 水污染 0.186477211628
# 生活 0.170049997544</code></pre>
<pre class="python"><code>#基于TextRank
keywords = jieba.analyse.textrank(content,topK = 5,withWeight = True,allowPOS = (&#39;n&#39;,&#39;nr&#39;,&#39;ns&#39;))
for item in keywords:
    print item[0],item[1]
    
#基于TextRank结果：
# 饮用水 1.0
# 美国 0.570564785973
# 奚传武 0.510738424509
# 单位 0.472841889334
# 讲座 0.443770732053</code></pre>
</div>
</div>
<div id="写在文后" class="section level2">
<h2>写在文后</h2>
<pre><code>自然语言处理（NLP）在数据分析领域有其特殊的应用，在R中除了jiebaR包，中文分词Rwordseg包也非常常用。一般的文本挖掘步骤包括：文本获取（主要用网络爬取）——文本处理（分词、词性标注、删除停用词等）——文本分析（主题模型、情感分析）——分析可视化（词云、知识图谱等）。本文是自然语言处理的第一篇，后续将分别总结下应用深度学习Word2vec进行词嵌入以及主题模型、情感分析的常用NLP方法。</code></pre>
</div>
<div id="参考资料" class="section level2">
<h2>参考资料</h2>
<blockquote>
<ul>
<li>Introduction · jiebaR 中文分词<a href="https://qinwenfeng.com/jiebaR/segment.html" class="uri">https://qinwenfeng.com/jiebaR/segment.html</a></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>知乎：【文本分析】利用jiebaR进行中文分词<a href="https://zhuanlan.zhihu.com/p/24882048" class="uri">https://zhuanlan.zhihu.com/p/24882048</a></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>雪晴数据网：全栈数据工程师养成攻略<a href="http://www.xueqing.tv/course/73" class="uri">http://www.xueqing.tv/course/73</a></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>搜狗实验室，词性标注应用<a href="http://www.sogou.com/labs/webservice/" class="uri">http://www.sogou.com/labs/webservice/</a></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>【R文本挖掘】中文分词Rwordseg <a href="http://blog.163.com/zzz216@yeah/blog/static/162554684201412895732586/" class="uri">http://blog.163.com/zzz216@yeah/blog/static/162554684201412895732586/</a></li>
</ul>
</blockquote>
</div>
